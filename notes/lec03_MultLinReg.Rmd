---
title: "Multiple Linear Regression Analysis"
author: "MATH 456 - Spring 2016"
subtitle: Robin Donatello
output:
  html_document:
    highlight: pygments
    theme: spacelab
    toc: yes
    toc_float: yes
  pdf_document: default
---
```{r,echo=FALSE, warning=FALSE, message=FALSE}
library(knitr); library(rmarkdown);library(ggplot2)
opts_chunk$set(warning=FALSE, message=FALSE, fig.height=4, fig.width=5) 
```

```{r results="asis", echo=FALSE}
source("C:/Github/MATH456/stylesheets/custom.R")
```

Navbar: [[Home]](../index.html) [[Schedule]](../Schedule.html) [[Data]](../data/Datasets.html) [[Week 4 Overview]](../wk04.html) [[HW Info]](../HW_Info.html)  [[Google Group]](https://groups.google.com/forum/#!forum/csuc_stat)  [[Google Drive]](https://drive.google.com/a/mail.csuchico.edu/folderview?id=0B83Z8_sNw3KPcnVrYzVFRHUtcHM&usp=sharing)

# Multiple Regression

## Associated Reading
Afifi: Chapter 7

## Aims
* Extend simple linear regression.
* Describe linear relationship between a single continuous $Y$ variable, 
  and several $X$ variables.
* Draw inferences regarding this relationship.
* Predict $Y$ from $X_{1}, X_{2}, \ldots , X_{P}$.

Now it's no longer a 2D regression _line_, but a $p$ dimensional 
regression plane. 

![Figure 7.1](regression_plane.png)

## Types of X variables
* Fixed: The levels of $X$ are selected in advance with the intent to measure
  the affect on an outcome $Y$. 
* Variable: Random sample of individuals from the population is taken and $X$
  and $Y$ are measured on each individual. 
* X's can be continuous or discrete (categorical)
* X's can be transformations of other X's, e.g., polynomial regression

## Mathematical Model

$$ y_{i} = \beta_{0} + \beta_{1}x_{1i} + \ldots + \beta_{p}x_{pi} + \epsilon_{i}$$

The Gauss-Markov assumptions still hold here. Recall these concern the set of error random variables, $\epsilon_{i}$:  

* They have mean zero  
* They are homoscedastic, that is all have the same finite variance: $Var(\epsilon_{i})=\sigma^{2}<\infty$  
* Distinct error terms are uncorrelated: (Independent) $\text{Cov}(\epsilon_{i},\epsilon_{j})=0,\forall i\neq j.$  

The regression model relates $y$ to a function of $\textbf{X}$ and $\mathbf{\beta}$, 
where $\textbf{X}$ is a $nxp$ matrix of $p$ covariates on $n$ observations and 
$\mathbf{\beta}$ is a length $p$ vector of regression coefficients. In matrix notation
this looks like: 

$$ \textbf{y} = \textbf{X} \mathbf{\beta} + \mathbf{\epsilon} $$

### Parameter Estimation
The goal of regression analysis is to minimize the residual error. 
That is, to minimize the difference between the value of the dependent
variable predicted by the model and the true value of the dependent variable.

$$ \epsilon_{i} = \hat{y_{i}} - y_{i}$$

The method of least squares accomplishes this by finding parameter estimates 
$\beta_{0}$ and $\beta_{1}$ that minimized the sum of the squared residuals:

$$ \sum_{i=1}^{n} \epsilon_{i} $$

For simple linear regression these are found to be 
$$ \hat{\beta_{0}} = \bar{y} * \hat{\beta_{1}}\bar{x} \quad \mbox{  and  } \quad  \hat{\beta_{1}} = r\frac{s_{y}}{s_{x}} $$

For multiple linear regression the function to minimize is

$$ \sum_{i=1}^{n} |y_{i} - \sum_{i=1}^{p}X_{ij}\beta_{j}|^{2}$$

Or in matrix notation

$$ || \mathbf{y} - \mathbf{X}\mathbf{\beta} ||^{2} $$ 

The details of methods to solve these minimization functions are left to a
course in mathematical statistics, however we will return to this notation. 

### Continued Example: Lung Function

In Chapter 6 the data for fathers from the lung function data set were 
analyzed. These data fit the variable-X case. Height was used as the 
$X$ variable in order to predict `FEV`. 

```{r}
fev <- read.delim("C:/GitHub/MATH456/data/Lung_020716.txt", sep="\t", header=TRUE)
summary(lm(FFEV1 ~ FHEIGHT , data=fev))
round(confint(lm(FFEV1 ~ FHEIGHT , data=fev)),2)
```
This model concludes that FEV1 in fathers significantly increases by 0.12 (95% CI:
0.09, 0.15) liters per additional inch in height (p<.0001). Looking at the 
multiple $R^{2}$ (correlation of determination), this simple model explains 
25% of the variance seen in the outcome $y$. 

However, FEV tends to decrease with age for adults, so we should be able
to predict it better if we use both height and age as independent variables
in a multiple regression equation. 

First let's see different ways to graphically explore the relationship between
three characteristics simultaneously.

#### 3D scatterplots
```{r}
library(scatterplot3d)
scatterplot3d(x=fev$FHEIGHT, y=fev$FAGE, z=fev$FFEV1, 
              xlab="Height", ylab="Age", zlab="FEV", 
              main="Relationship between FEV1, height and age for fathers.")
```

See http://www.statmethods.net/graphs/scatterplot.html for two simple ways to
create interactive, spinning 3D scatterplots. 

#### Controlling the color, or size of points using the third characteristic
```{r, fig.width=10}
library(gridExtra)
a <- qplot(y=FFEV1, x=FAGE, color=FHEIGHT, data=fev)
b <- qplot(y=FFEV1, x=FHEIGHT, size=FAGE, data=fev)
grid.arrange(a, b, ncol=2)
```

The scatterplot of FEV against age demonstrates the decreasing trend of
FEV as age increases, and the increasing trend of FEV as height increases. 
The third color however is pretty scattered across the plot. There is no
obvious trend observed.

*  What direction do you expect the slope coefficient for age to be? For height? 

## Model fitting

### Simple Linear Regression
Let's examine the bivarate relationship of FEV1 (forced expiratory volume in 1 minute)
for fathers `FFEV1` on their age `FAGE`. 

```{r}
summary(lm(FFEV1 ~ FAGE, data=fev))
```

For every one year older the father gets, his FEV1 significantly decreases by 
0.03 liters (p = .00001).

```{r}
summary(lm(FFEV1 ~ FHEIGHT, data=fev))
```
For every inch taller a father is, his FEV1 significantly increases by 
0.11 liters (p < .0001).

### Multiple Linear Regression
Fitting a regression model in R with more than 1 predictor is trivial. Just add
each variable to the right hand side of the model notation connected with a `+`. 

```{r}
mv_model <- lm(FFEV1 ~ FAGE + FHEIGHT, data=fev)
summary(mv_model)
```
A father who is one year older is expected to have a FEV value 0.03 liters 
less than another father of the same height (p<.0001).

A father who is the same age as another father is expected to have a FEV
value of 0.11 liter greater than another father of the same age who is one inch shorter (p<.0001). 

For the model that includes age, the coefficient for height is now 
`r round(mv_model$coefficients[3],2)`, which is interpreted as the rate of change 
of FEV1 as a function of height **after adjusting for age**. 
This is also called the **partial regression coefficient** of FEV1 on height after 
adjusting for age. 

Both height and age are significantly associated with FEV in fathers (p<.0001 each).

### ANOVA for regression
For a global test to see whether or not the regression model is helpful in 
predicting the values of $y$, we can use an ANOVA. This is the same as testing
that all $\beta_{j}, j=1, \ldots, p$ are all equal to 0. Let's look at what
we get if we wrap the ANOVA function, `aov()` around the linear model results. 

```{r}
summary(aov(mv_model))
```

We get the sums of squares (SS) for each predictor individually, not combined
into a SS for regression and a SS residuals. So where do we find this
global test that this model is better than using no predictors at all? 
At the very last line in the summary of the linear model results. 

```{r}
summary(mv_model)
```

## Predictions

What is the predicted FEV1 for a 30-year-old male whose height was 70 inches?
There are multiple ways to evaluate this calculation. 

1. Manual calculation by plugging in the value of each variable individually
```{r}
betas <- mv_model$coefficients
betas
betas[1] + betas[2]*30 + betas[3]*70
```

_Interpretation:_ We would expect a a 30-year-old male whose height was 70
inches to have an FEV1 value of 4.45 liters.

2. In matrix notation a new vector `x.new` is created and multiplied
by the vector of coefficients. 
```{r}
x.new <- c(1, 30, 70)
x.new %*% betas
```

3. Using the `predict()` function. This requires the `newdat` input to be a
a `data.frame` object. 
```{r}
x.pred <- data.frame(cbind(FAGE = 30, FHEIGHT = 70))
predict(mv_model, newdata=x.pred)
```

Compare this value to the single predictor equation with just height:
```{r}
slr.mod <- lm(FFEV1 ~ FHEIGHT, data=fev)
predict(slr.mod, newdata = data.frame(FHEIGHT=70))
```

This value of `r round(predict(slr.mod, newdata = data.frame(FHEIGHT=70)),2) `
is the rate of change of FEV1 for fathers as a function of height when no other
variables are taken into account. 

## Comparing coefficients


* Problem: Values of $\hat{\beta_{j}}$ are NOT directly comparable for any $j=1, \ldots, p$. 
* Solution: Standardized coefficients:
$$\hat{\beta}_{j} = \hat{\beta}_{j} \frac{s_{x_{j}}}{s_{y}}$$
* These coefficients are the same as if you standardized the data X and
  Y prior to conducting the regression. 
* The larger the magnitude of the standardized coefficient, 
  the more $X_{i}$ directly contributes to the prediction of $y$. 
* The standardized slope coefficient is the amount of change in the mean of
  the standardized $y$ values when the value of $X$ is increased by one
  standard deviation, keeping the other $X$ variables constant. 


#### Interlude: Reporting regression coefficients using tables in Markdown
There are not many options for making readable tables in Markdown.
For better control you would be best suited by switching to LaTeX and
Sweave (both can be done in R Studio). Here is a reference webpage
that shows three basic options. 
http://kbroman.org/knitr_knutshell/pages/figs_tables.html 

Here is an example using `xtable`. 
I first round the results of the table to 2 digits, then format the
p-value to read <.0001 instead of a super small digit. Then I wrap
`xtable()` around that output. 
```{r, results="asis"}
library(xtable)
options(xtable.comment=FALSE)
coeff.out <- round(summary(mv_model)$coef, 2)
coeff.out[,4] <- ifelse(coeff.out[,4]<.0001, "<.0001", coeff.out[,4])
tab <- xtable(coeff.out)
names(tab)[3:4] <- c("t", "p-value")
print(tab, type="html")
#print(tab) #latex printing only
```

_R specifics_:  Rounding has to come first because the `ifelse()` changed the 
data type in the entire matrix from numeric to character, and once it's a 
character you can't round. There are ways around this but I am not going
to discuss it here. 


## Estimated Covariance

For the variable-X case, several additional parameters are needed to characterize
the full joint distribution $f(x, y)$. A matrix of the estimated covariances 
between the parameter estimates $\beta{j}$'s can be obtained in R by using the `vcov()` 
function on the model output. 

```{r}
round(vcov(mv_model), digits=3)
```

**Note:** This is NOT the same as the variance-covariance matrix of the actual
data. This can be found using the variance `var()` function. For the sake of 
example I put a third covariate in the list: `FWEIGHT`. 

```{r}
library(dplyr)
x.vars <- fev %>% select(FAGE, FHEIGHT, FWEIGHT, FFEV1) # requires dplyr
round(var(x.vars), digits=3)
```

* The variances $\sigma^{2}_{1}, \sigma^{2}_{2}, \ldots, \sigma^{2}_{p}$ are
  found down the diagonal.
* The covariances $\sigma_{ij}$ are found on the off diagonal entries.  
* This matrix is symmetric

Similarly the correlation matrix can be found using the `cor()` function.
```{r}
round(cor(x.vars), digits=3)
```

* The correlation of a variable with itself is always 1, hence the 1's down the
  diagonal.
* Each entry on the off diagonal is the simple correlation value: $\rho_{12}$. 
* This matrix is also symmetric. 




## Model Diagnostics 

The same set of regression diagnostics can be examined to identify
any potential influential points, outliers or other problems with
the linear model. 

```{r}
par(mfrow=c(2,2))
plot(mv_model)
```

The textbook provides more details about tools to detect 
outliers and influential points by examining the following
measures:

* studentized residuals
* leverage
* DFFITS
* Cooks distance

At least one reference website on how to visualize these 
measures can be found here: 
http://www.statmethods.net/stats/rdiagnostics.html


### Multicollinearity

* Occurs when some of the X variables are highly intercorrelated.
* Affects estimates and their SE's (p. 143)
* Look at tolerance, and its inverse, the Variance Inflation Factor (VIF)
* Need tolerance < 0.01, or VIF > 100.

```{r}
library(car)
vif(mv_model)
tolerance = 1/vif(mv_model)
tolerance
```

* Solution: use variable selection to delete some X variables.
* Alternatively, use Principal Components (Ch. 14)

## Interaction Models

Consider a model with only two predictors: $X_{i}$ and $x_{j}$.

$$E(y|x_{i}) = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} $$

A model of this form is said to be _additive_. If the additive terms for 
these variables do not completely specify their effects on the dependent 
variable $Y$, then interaction of $X_{1}$ and $X_{2}$ is said to be present. 

#### Interlude: Reshaping the Lung data from wide to long. 

The data on Lung function came to us in _wide_ format, with separate 
variables for mother's and father's FEV1 score (`MFEV1` and `FFEV`). 
To analyze the effect of gender on FEV, the data need to be in _long_
format, with a single variable for `FEV` and a separate variable for
gender. The following code chunk demonstrates one method of combining
data on height, gender, age and FEV1 for both males and females. 

```{r}
fev2 <- data.frame(gender = c(fev$FSEX, fev$MSEX), 
                   FEV = c(fev$FFEV1, fev$MFEV1), 
                   ht = c(fev$FHEIGHT, fev$MHEIGHT), 
                   age = c(fev$FAGE, fev$MAGE))
fev2$gender <- factor(fev2$gender, labels=c("M", "F"))
head(fev2)  
```

What does the relationship between height and FEV look like for the overall sample? 
```{r}
ggplot(data=fev2, aes(x=ht, y=FEV)) + geom_point() + xlab("Height") + 
  ggtitle("Scatterplot of FEV vs height of adults") + 
  geom_smooth(se=FALSE, method='lm', col="purple") + 
  geom_smooth(se=FALSE, col="brown") 
```

However if we examine the relationship separately by gender we see
a slightly different story. 
```{r}
ggplot(data=fev2, aes(x=ht, y=FEV, col=gender)) + geom_point() + xlab("Height") + 
  ggtitle("Scatterplot of FEV vs height of adults by gender") + 
  geom_smooth(se=FALSE, method='lm') + 
  geom_smooth(aes(x=ht, y=FEV), col="purple", se=FALSE, method='lm')
```

If we put numeric summaries to these bivariate relationships we see that the
correlation between FEV and height overall is
`cor(fev2$FEV, fev2$ht)` = `r round(cor(fev2$FEV, fev2$ht),2)`, but for
males it is
`cor(fev$FFEV, fev$FHEIGHT)` = `r round(cor(fev$FFEV, fev$FHEIGHT),2)`, 
and for females the correlation is
`cor(fev$MFEV, fev$MHEIGHT)` = `r round(cor(fev$MFEV, fev$MHEIGHT),2)`.


**Conclusion:** The relationship between FEV1 and height may depend on gender. 
These variables are said to **interact** with each other. In other words, 
gender changes the relationship between height and FEV1. 

Consider the linear model of FEV on gender($x_{1}$), height($x_{2}$) and age($x_{3}$)
where gender interacts with  height. 

$$ FEV1 \sim \beta_{0} + \beta_{1}*gender + \beta_{2}*height + \beta_{3}*age + \beta_{4}*gender*height $$

According to the codebook, when `gender = 0` the record is on a male, and when `gender = 1` the record is
on a female. The model for males then simplifies to: 

$$ FEV1 \sim \beta_{0} + \beta_{2}*height + \beta_{3}*age $$

and the model for females would be:

$$ FEV1 \sim (\beta_{0} + \beta_{1}) + (\beta_{2} + \beta_{4})*height + \beta_{3}*age$$

Interactions are fit in `R` by simply multiplying `*` the two variables together in the model statement. 
```{r}
intx.model <- lm(FEV ~ gender + ht + age + gender*ht, data=fev2)
summary(intx.model)
confint(intx.model)
```

* Using the output from R after fitting the interaction model specified above, write out
the regression equation for males and females separately. 


The first thing to notice is the coefficient label: `genderF`. Since the
variable for gender is a factor variable, is automatically _reference coded_ 
into a new variable that is not present on the data set but only as part of the 
linear model function. This variable `genderF` is a 1 if the gender on record 
is female, and 0 otherwise. 

The p-value for the interaction term `ht:genderF` is large, and the confidence
interval for this parameter covers zero, so there is no indication that an 
interaction exists. That is, there is not enough reason to believe that gender
significantly affects the relationship between height and FEV. 

**Reminder** Just as in a Two-Way ANOVA with an interaction term, the main effects
cannot be interpreted directly when there is an interaction in the model. This means
you **cannot** interpret the direct effect of height or gender on FEV1 using just 
$\beta_{1}$ or $\beta_{3}$.


* What is the effect of height on FEV1 for females? 


Later on we will talk about how categorical variables with multiple factor
levels are reference coded for entry into models. 


## Stratification

Sometimes it is desirable to examine equations for subgroup of
the population. Consider the relationship between age, height and FEV
by gender. We write the model with the same set of covariates on each 
strata (gender). 

$$ FEV1_{M} \sim \beta_{0M} + \beta_{1M}*height + \beta_{2M}*age $$
$$ FEV1_{F} \sim \beta_{0M} + \beta_{1F}*height + \beta_{2F}*age $$

Example: Within each gender there exists a negative correlation between 
age and height. However in the combined sample this appears to be a
positive correlation. 


```{r}
ggplot(data=fev2, aes(x=age, y=ht, col=gender)) + geom_point() +
  ggtitle("Scatterplot of height vs age of adults by gender") + 
  geom_smooth(se=FALSE, method='lm') + 
  geom_smooth(aes(x=age, y=ht), col="purple", se=FALSE, method='lm')
```

This is similar to [Simpsons Paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox), 
where there are a number of additional examples of this situation including the 
[UC Berkeley gender bias lawsuit](http://vudlab.com/simpsons/).

Since gender affects the relationship between FEV and both height and age, the
appropriate model would then be:

$$ FEV1 \sim \beta_{0} + \beta_{1}*gender + \beta_{2}*height + \beta_{3}*age + \beta_{4}*gender*height + \beta_{5}*gender*age $$

If we let gender = 0 if the record is on a male, and gender = 1 if the record is
on a female, then the model for males would be:

$$ FEV1 \sim \beta_{0} +  \beta_{2}*height + \beta_{3}*age $$

and the model for females would be:

$$ FEV1 \sim (\beta_{0} + \beta_{1}) + (\beta_{2} + \beta_{4})*height + (\beta_{3}+\beta_{5})*age$$

Instead of running the model on the full set of data and then calculating
the correct coefficient for each gender we _stratify_ the model by fitting 
the model on each subgroup separately. 


```{r}
# subset the data
M <- subset(fev2, gender=="M")
F <- subset(fev2, gender=="F")
# Overall model
overall_model <- lm(FEV ~ age + ht, data=fev2)
# run stratified models
male_model   <- lm(FEV ~ age + ht, data=M)
female_model <- lm(FEV ~ age + ht, data=F)
```

The overall model indicates that FEV decreases with age and increases
with height (p<.0001 each)
```{r, results='asis'}
tab <- xtable(summary(overall_model), digits=3)
print(tab, type="html")
#print(tab)
```

Model on females only:
```{r, results='asis'}
tabf <- xtable(summary(female_model), digits=3)
print(tabf, type="html")
#print(tabf)
```

Model on males only:
```{r, results='asis'}
tabm <- xtable(summary(male_model), digits=3)
print(tabm, type="html")
#print(tabm)
```

Create a prediction of FEV for two cases, a 30 year old who is
66cm tall, and a 50 year old who is 62cm tall. 

```{r, results='asis'}
# create new data frame using the overall mean height
new.data <- data.frame(ht = c(66, 62), age=c(30, 50))
# predict on each model
o.pred <- predict(overall_model, new.data)
m.pred <- predict(male_model, new.data)
f.pred <- predict(female_model, new.data)
pred <- xtable(rbind(o.pred, m.pred, f.pred), digits=3)
print(pred, type="html")
#print(pred)
```

Column number 1 is a prediction of FEV for a 30 year old who is 66cm tall,
column number 2 is a prediction of FEV for a 50 year old who is 66cm tall. 
Notice the overall prediction

Even though the equations for males and females look quite similar, the 
predicted FEV1 for females of the same height and age as a male is expected 
to be less. The overall prediction is right in between the two estimates. 

_Note of caution: Stratification implies that the stratifying
variable interacts with all other variables._ 

### Testing equality of individual coefficients between groups
To compare the regression coefficients for men and women we could simply
compare the sign and magnitude of the standardized regression coefficients. 

If an interaction exists, then the two coefficients from the stratified models
would be equal. 

To test the null hypothesis that the effect of height on FEV is the same
across genders, we compute the following test statistic:

$$ Z = \frac{\beta_{2M} - \beta_{2F}}{\sqrt{Var(\beta_{2M}) +Var(\beta_{2F})}}$$

This $Z$ statistic follows the standard normal distribution, $\mathcal{N}(0,1)$ 
and so you can use the `pnorm()` function to calculate the p-value for the test. 

$$ Z = \frac{0.114397 - 0.092593}{\sqrt{0.015789^2 +0.013704^2}} $$

```{r}
z = (0.114397 - 0.092593)/(sqrt(0.015789^2 + 0.013704^2))
z
2*(1-pnorm(z))
```

_`pnorm(z)` takes the value of the test statistic as the argument `z`, and
returns the probability of a random variable being **below** the test
statistic. This can be thought of as the area to the **left** under the 
normal probability distribution curve. The p-value for a statistical test 
is the probability of observing a test statistic equal to or greater than 
the one observed. Since our test statistic is positive, we are interested 
in P(Z>1.04). Thus we want to calculate the area is the **right** under
the normal probability distribution. Since the result of `pnorm` gives
us the left and we want the right, and the full area under the curve 
adds up to 1, we simply calculate `1-pnorm()` to find the area to
the right. Lastly, since this is a two tailed test we double 
the tail area to calculate the p-value of the test in question_

With a large p-value of 0.30 there is insufficient evidence to believe
that the relationship between FEV and height differs by gender. 


#### Testing using a CI
You can also attempt to test for a difference in slope coefficients by
comparing the confidence intervals for the parameters. _Note: I am showing
how I wrangled the linear model output into a table that is easily read for
your info only. This is not mandatory but it looks nice._

```{r, results="asis"}
mci <- paste("(", round(confint(male_model)[,1],2),", ", 
             round(confint(male_model)[,2],2), ")", sep="")
fci <- paste("(", round(confint(female_model)[,1],2),", ", 
             round(confint(female_model)[,2],2), ")", sep="")
out <- cbind(Male = mci, Female = fci)[-1,]
rownames(out) <- c("Age", "Height")
out <- xtable(out)
print(out, type="html")
#print(out)
```

* If CIs _do not_ overlap then slopes are significantly different from each other.
* Since CIs do overlap, then two slopes _may or may not_ be significantly
  different from each other.

For both age and height the CI's for the slopes overlap a large amount, so I
would suspect that there is no significant difference in the coefficients
between models. This finding corroborates the formal statistical test
done earlier. 

## What to watch out for

* See cautions for simple regression including violations of assumptions, outliers, 
  influential points
* Need representative sample
* Multicollinearity: coefficient of any one variable can vary widely, 
  depending on what others are included in the model
* Missing values: Even more important here
    - Default method is complete case analysis
    - If any variable in the model has missing data, the entire record is 
      excluded.
* Number of observations in sample should be large enough relative
  to the number of parameters that are being estimated.
  - This includes the variances and covariances of the parameter 
    estimates. 


##### On Your Own

1. Fit the regression model for the fathers using `FFVC` as the 
   dependent variable and age and height as the independent variables.
   Write the results for this regression model so they would be suitable
   for inclusion in a report. Include a table of results. 


2. Confirm that this F-test in the model results is the correct one to 
   use by manually calculating the F statistic using an ANOVA table. 
   Confirm the degrees of freedom in both the numerator and denominator 
   are correct, as well as the calculation of the p-value. If you are
   not familiar with the `qf()` function in R to find the probability 
   under the F distribution, here are some helpful resources in addition 
   to your classmates. 
    - http://www.r-tutor.com/elementary-statistics/probability-distributions/f-distribution
    - https://www.youtube.com/watch?v=PZiVe5DMJWA
  
  
3. Fit a regression model for females using `MFVC` as the dependent 
   variable and age and height as the independent variables. Summarize 
   the results in a tabular form. 
   
4. Test whether the effect of age and height on FVC for males are 
   significantly different than for females. 

5. Using the data on births from North Carolina (`NCbirths`), create a model
   of the weight of the baby at birth in pounds using the mothers age, 
   smoking habit, and the number of hospital visits during pregnancy as dependent
   variables. Interpret the regression coefficients in context of the problem 
   and include 95% confidence intervals and p-values in your discussion. 

6. Find a 95% prediction interval for a 30-year-old smoking mother with 16 visits 
   to the doctor during her pregnancy. 
   
7. Test for an interaction between smoking habit and the mothers age. 
   Include a plot similar to the one shown in the lecture notes to support 
   your findings. 


8. Using the Parental HIV data, generate a variable that represents the sum
   of the variables describing the neighborhood where the adolescent lives
   (`NGHB1-NGHB11`). Is the age at which adolescents start smoking different
   for girls compared to boys, after adjusting for the score describing the 
   neighborhood? 
   
   
# Categorical Predictors

## Associated  Reading

Afifi: Chapter 9.3, Harrel Ch 2

## Factor variable coding

* Most commonly known as "Dummy coding"
* Better used term: Indicator variable
* Math notation: **I(gender == "Female")**. 
* A.k.a reference coding
* For a nominal X with K categories, define K indicator variables.
    - Choose a reference (referent) category:
    - Leave it out
    - Use remaining K-1 in the regression.
    - Often, the largest category is chosen as the reference category.




## Example: Religion against income and depression
Consider a log-linear model for the effect of marital status ($X_2$) on
log income while controlling for age($X_1$). This is called a log-linear
model because the outcome has been log transformed. 

$$ log(Y_i) = \beta_0 + \beta_1*x_1 + \beta_2*x_2 $$

Forget why we log-transformed income? Re-visit the notes on Data Screening 
and Transformations http://norcalbiostat.github.io/MATH456/notes/lec01_data_prep.html 

```{r}
dep <- read.table("C:/GitHub/MATH456/data/Depress_041616.txt", sep="\t", header=TRUE)
names(dep) <- tolower(names(dep)) # I hate all captal variable names
levels(dep$marital)
```

Marital status has 5 levels, so we would need 4 indicator variables. 
R always uses the first level of a factor variable as the reference level. 

* Let $x_{2}=1$ when `marital='Married'`, and 0 otherwise,  
* let $x_{3}=1$ when `marital='Never Married'`, and 0 otherwise,  
* let $x_{4}=1$ when `marital='Separated'`, and 0 otherwise,  
* let $x_{5}=1$ when `marital='Widowed'`, and 0 otherwise. 

The mathematical model would look like: 

$$ log(Y)|X \sim \beta_{0} + \beta_{1}*x_{1} + \beta_{2}x_{2} + \beta_{3}x_{3} + \beta_{4}x_{4} + \beta_{5}x_{5} $$

Two levels of interpretation here. 

1. The outcome is log transformed, so the interpretation has to be 
   back-transformed.   
2. The coefficients for the other levels of the categorical variable
   are in _comparison_ to the reference level. 


**Interpretation of log-linear models** 
Calculate the change in $Y$ that corresponds to a one unit change in $x_1$. 
Since marital status is remaining constant, I will exclude it from the 
calculations below to save space and not to detract from the main point.

Write each equation down

$$ log(Y)|x_1 = \beta_{0} + \beta_{1}x_{1}$$ 
$$ log(Y)|(x_1+1)  = \beta_{0} + \beta_{1}(x_{1}+1)$$ 

Find the difference

$$ (log(Y)|x_1) - (log(Y)|(x_1+1)) = (\beta_{0} + \beta_{1}x_{1}) - (\beta_{0} + \beta_{1}(x_{1}+1))$$ 

and simplify. 

$$ log(\frac{Y|x_1}{Y|x_1+1}) = \beta_{1}$$ 
$$ \frac{Y|x_1}{Y|x_1+1} = e^{\beta_{1}}$$

Each 1-unit increase in $x_{j}$ multiplies the expected value of Y by $e^{\hat{\beta_{j}}}$.  

Interpretation: $100\hat{\beta_{j}}$ is the expected **percentage** change
in $Y$ for a unit increase in $x_{j}$.


The nice thing about factor variables in R, is that the appropriate 
indicator variables are automatically created for you by the linear
model (`lm()`) function.

```{r, echo=1}
summary(lm(log(income) ~ age + marital,data=dep))
cf <- round(exp(coef(summary(lm(log(income) ~ age + marital,data=dep)))),2)
```


* For every year older, a persons income decreases by 1%. (`exp(-0.009)` = `r cf[2,1]`)
* Married individuals have a 52% higher income compared to those who are divorced. (`exp(-0.417)` = `r cf[3,1]`)
* Those who have never been married have  16% lower income compared to those who are divorced. (`exp(-0.183)` = `r cf[4,1]`)
* Separated individuals have 32% lower income compared to those who are divorced. (`exp(-0.394)` = `r cf[5,1]`)
* Widowed individuals have 24% lower income compared to those who are divorced. (`exp(-0.278)` = `r cf[6,1]`)

Other references on how to interpret regression parameters when they have 
been log transformed: 

* http://www.ats.ucla.edu/stat/mult_pkg/faq/general/log_transformed_regression.htm 
* http://www.kenbenoit.net/courses/ME104/logmodels2.pdf

## On Your Own
##### On Your Own

**Create a model to analyze the relationship of education status to depression level as measured by
CESD after controlling for age. Combine all education levels below a HS graduate into one reference 
category called "Up to HS" prior to analysis.**

This is a seemingly simple request, but there are a lot of steps you must do
to correctly analyze this question. 

1. Ensure that you are using the analyzable version of the depression data set. 
   It may be helpful to confirm that your recodes are correct by comparing
   your data management code file to mine [dm_depress](../data/dm_depress.html) 
   located on our course website. 
2. Reference your Ch3 homework (or the [solutions](./solutions/ch3_solutions.html))
   if you need help collapsing educational categories. 
3. Ensure that R is treating "Up to HS" as the reference category for education level. 
   If it is not, use the `levels` argument of the `factor()` function to reorder
   your factor levels. This is also presented in the Ch3 solutions. 
4. Consider a transformation of `CESD`. Explain and justify using graphical
   measures why you chose to, or chose not to, transform CESD prior to modeling. 
5. Check the model fit by examining the residuals to see if the assumption that 
   $\epsilon_{i} \sim \mathcal{N}(0, \sigma^{2})$ is upheld. 
6. Identify any potential outliers. Explain why you think they are outliers. 
   Examine their standardized residuals and leverage values.If any seem to stand out or 
   have high values for either measure, exclude them from the analysis and re-run the model. 
7. Once you have finalized your model, interpret ALL coefficients in context of the problem. 
   State if any are significantly predictive of the outcome, provide p-values in your conclusion. 
8. Does this model do well at all in predicting CESD? Answer this question using both the
   coefficient of determination and the ANOVA test of overall global fit (testing that
   all $\beta$'s are 0)
   


# Splines & other non-linear terms

## Associated Reading

* Afifi Section 9.4
* Harrell 2.4.3, pg 39 http://biostat.mc.vanderbilt.edu/tmp/course.pdf
* Harrell ch2 from second edition [pdf](https://drive.google.com/open?id=0B83Z8_sNw3KPb0dsYzk0OTR1Nms) in shared GDrive. 
* https://www.youtube.com/watch?v=o_d4hmKhmsQ
* http://www.r-bloggers.com/thats-smooth/


#### Example 1: Simulated data.
Example data pulled from 
_http://faculty.washington.edu/heagerty/Courses/b571/homework/spline-tutorial.q_

Suppose we have a predictor that takes the values 1:24
```{r}
x <- c(1:24)
```
and there is an outcome variable that is predicted by
the variable X, but in some non-linear fashion:
```{r}
mu <- 10 + 5 * sin( x * pi / 24 )  - 2 * cos( (x-6)*4/24 )
```
But there is always some amount of error associated with real data.
```{r}
set.seed(42)
eee <- rnorm(length(mu))
```
So our simulated data then is the true trend + the noise. 
```{r}
y <- mu + eee
```

Let's look at the data, and the real mean trend without the random noise. 
```{r}
plot(y~x)
lines(x, mu, col="red" )
```

Let's look at ways to fit a model to this data. 


## Linear
Ignore the trend and fit a linear model. 

$$ E(Y|X) = \beta_{0} + \beta_{1}X$$ 

```{r}
fit.slr <- lm(y~x)
plot(y~x)
abline(fit.slr)
lines(x, mu, col="red" )
```

Undoubtedly not a good fit. Examining the residuals shows the non-constant
variance clearly. 

```{r}
par(mfrow=c(2,2))
plot(fit.slr)
```

## Piecewise linear splines
We allow the $x$ axis to be divided into intervals, with a linear model
fit within each interval. The breakpoints between intervals are called _knots_. 
This is where you are allowing the slope of the line to change. 
For example to break the x-axis into three sections we would use 2 knots. 
The model would look like. 

$$ E(Y|X) = \beta_{0} + \beta_{1}X + \beta_{2}(X-a)_{+} + \beta_{3}(X-b)_{+}$$

where $(u)_{+}$ contains the value of $u$ when $u$ is positive, and 0 otherwise. 

Let's put knots at 6, 12, and 18. 
```{r}
x6 <- (x-6)
x6[ x6<0 ] <- 0

x12 <- (x-12)
x12[ x12<0 ] <- 0

x18 <- (x-18)
x18[ x18<0 ] <- 0
```
What does this data look like now? 
```{r}
t(cbind(x, x6, x12, x18)[8:20,])
```

Now let's fit this model. 
```{r}
fit.lin.spline <- lm(y ~ x + x6 + x12 + x18)
plot(y~x)
lines(x, predict(fit.lin.spline), col="orange")
points(c(6, 12, 18), predict(fit.lin.spline)[c(6, 12, 18)], pch=16, col="orange")
lines(x, mu, col="red" )
```

Much closer than the linear model, but it still lacks the curvature that 
is present in the data. The residual plots look much better already. 

```{r}
par(mfrow=c(2,2))
plot(fit.lin.spline)
```


## Powers
A non-linear effect can be as simple as adding a covariate at some power. 

$$ E(Y|X) = \beta_{0} + \beta_{1}X + \beta_{2}X^{2}$$ 

Testing $H_{0}: \beta_{2} = 0$ tests the null hypothesis that the effect
of $X_1$ on $Y$ is linear vs the effect is quadratic. 

```{r}
x.squared <- x^2
fit.sq <- lm(y~x + x.squared)
plot(y~x)
lines(x, predict(fit.sq), col="blue")
lines(x, mu, col="red" )
```

A cubic term could also be added. 

$$ E(Y|X) = \beta_{0} + \beta_{1}X + \beta_{2}X^{2} + \beta_{3}x^{3}$$ 

```{r}
x.cubed <- x^3
fit.cubic <- lm(y~x + x.squared + x.cubed)
plot(y~x)
lines(x, predict(fit.cubic), col="purple")
lines(x, mu, col="red" )
```

Adding this cubic term allows for another "wiggle" in the fitted line. 

## Cubic splines
Combining the two concepts allows for a very flexible polynomial model. 

$$ E(Y|X) = \beta_{0} + \beta_{1}X + \beta_{2}X^{2} + \beta_{3}X^{3} + 
            \beta_{4}(X-a)^{3}_{+} + \beta_{5}(X-b)^{3}_{+}$$
            
Using the knots at 6, 12, and 18 let's fit a cubic spline. 

```{r}
x6.cubed <- x6^3
x12.cubed <- x12^3
x18.cubed <- x18^3

fit.cub.spline <- lm(y ~ x + x.squared + x.cubed + x6.cubed + x12.cubed + x18.cubed)
```

Replot and look at the fitted model. 
```{r}
plot(y~x)
lines(x, predict(fit.cub.spline), col="darkgreen")
points(c(6, 12, 18), predict(fit.cub.spline)[c(6, 12, 18)], pch=16, col="darkgreen")
lines(x, mu, col="red" )
```

It seems like our model is fitting the data better, but sometimes there is a 
balance between a flexible model, and overfitting the data (when your model fits
each point better than the true underlying average.)

## Natural splines
Also called _natural splines_, these models constrain the model to be linear in the
tails. The model is difficult to write, and fit by hand so we will use the `splines` 
package. 

```{r}
library(splines)

fit.ns = lm( y ~ ns(x, knots=c(6,12,18) ) )

plot(y~x)
lines(x, predict(fit.ns), col="darkcyan")
points(c(6, 12, 18), predict(fit.ns)[c(6, 12, 18)], pch=16, col="darkcyan")
lines(x, mu, col="red" )
```

There are other methods of model fitting under the umbrella of _Nonparametric Regression_, 
these include kernel smoothing, smoothing splines, and the familiar LOWESS 
(locally weighted scatterplot smoothing) and LOESS (Local regression) models. 

## On Your Own
##### On Your Own

1. Using the `cars` data set built into R, build a model to predict the
   distance a car takes to stop based on how fast it was going.  
2. Using the family lung function data, build a model to predict FEV1
   to height for the oldest child. 

   
   