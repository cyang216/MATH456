---
title: "Lec 05: Variable Selecdtion"
author: "MATH 456 - Spring 2016"
output:
  html_document:
    highlight: pygments
    theme: spacelab
    toc: yes
  pdf_document: default
---
```{r,echo=FALSE, warning=FALSE, message=FALSE}
library(knitr); library(rmarkdown);library(ggplot2)
opts_chunk$set(warning=FALSE, message=FALSE) 
```

```{r results="asis", echo=FALSE}
source("~/Github/MATH456/stylesheets/custom.R")
```

Navbar: [[Home]](../index.html) [[Schedule]](../Schedule.html) [[Data]](../data/Datasets.html) [[Week 6 Overview]](../wk06.html) [[HW Info]](../HW Info.html)  [[Google Group]](https://groups.google.com/forum/#!forum/csuc_stat)  [[Google Drive]](https://drive.google.com/a/mail.csuchico.edu/folderview?id=0B83Z8_sNw3KPcnVrYzVFRHUtcHM&usp=sharing)

# Assigned Reading
Afifi Chapter 8

Variable selection methods are used mainly in exploratory situations where many
indepndent variables have been measured and a final model explaining the 
dependent variable has not been reached. 

In Bioinformatics and other fields that use Machine Learning techniques this
technique of selecting variables is also known as _feature selection_. 

# Big Picture
To do variable selection you need:

* A general test, 
* Selection criteria, and
* A selection process.

Consider a model with $P$ variables and you want to test if $Q$ additional 
variables are useful.   
$H_{0}: Q$ additional variables are useless, i.e., their $\beta$’s all = 0  
$H_{A}: Q$ additional variables are useful

Ex: Y = FEV1, X1 = ht, X2 = age, X3 = ethnicity, X4 = location.   
Test H0: location does not matter.

# A General Test

## Likelihood Ratio (Deviance) Test
* Deviance = -2 log likelihood
* Under $H_{0}$, the _full model_, the deviance = $D_{0}, df_{0} = N-P-1$
* Under $H_{a}$, the _reduced model_, the deviance = $D_{a}, df_{a} = N-P-Q-1$
* LR (deviance) test statistic is:
* $D_{0} – D_{a}$ is distributed approximately as $\chi^{2}$ with $Q$ degrees of freedom
under $H_{0}$ for large $N$.

If we assume normally distributed residuals, the LR test becomes an exact $F$=test. 

$$F = \frac{(SSR_{red} - SSR_{full})/(df_{full} - df_{red})}{SSR_{full}/df_{full}} $$

#### Side track: Likelihood
Let $X$ be a random variable with pdf $f$ and that depends on the 
parameter $\theta$. The function 
$\mathcal{L}(\theta|x) = f_{\theta}(x)$ 
then is called the _Likelihood function_. It is the likelihood of
$\theta$ given the outcome $x$. Many analyses rely on maximizing this
function (Maximum likelihood estimate or MLE), but commonly do so 
by first taking the log of this function. Hence the _log likelihood_. 


# Selection Criteria

### Coefficient of determiniation
Was our model improved by the addition of this variable? Let's check the $R^{2}$. 
Recall this is calculated as the amount of variance explained by the model divided
by the total variance. 
```{r, eval=FALSE}
summary(aov(mv_model))
(16.05317+5.00380)/(16.05317+5.00380+42.04133)
```
This number is displayed as the **Multiple R-squared** value in the linear model 
output. But what about that other value, the **Adjusted R-squared**? Observe
what happens to these two variables when we put a variable into the model
that is not associated with the outcome, mothers weight.

```{r, eval=FALSE}
summary(lm(FFEV1 ~ FHEIGHT + FAGE + MWEIGHT, data=fev))
```

Adding more predictors to the model will always increase the $R^{2}$!

**Adjusted R^{2}**
$$ 1 - (1-R^{2})\frac{n-1}{n-p-1} $$


# Selection Process
We want to choose a set of independent variables that both will yield a good
prediction using as few variables as possible. 

## Manual
In many situations where regression is used, the investigator has strong
justification for including certain varibles in the model.

* previous studies
* accepted theory

The investigator may have prior justification for using certain variables but
may be open to suggestions for the remaining variables.

The set of independent variables can be broekn down into logical subsets

* The usual demographics are entered first (age, gender, ethnicity)
* A set of variables that other studies have shown to affect the dependent variable
* A third set of variables that _could_ be associated but the relationship has
  not yet been examined. 
  
Partially model-driven regression analysis and partially an exploratory analysis. 


## Automated



### Stepwise Regression
* Forward selection: $X$ variables added one at a time until optimal model reached
* Backward elimination: $X$ variables removed one at a time until optimal model reached
* Stepwise selection: combines the two

#### Forward selection
* Start with no $X$ variables
* Step 0: compute simple $R$’s of $Y$ with each $X_{i}$, and corresponding p-values.
* Step 1: enter variable with highest $|R|$ (smallest p-value), 
    - Compute partial $R$ (or $\beta$) of $Y$ with each other $X$, given $X$ in, 
      and corresponding p-values (p to enter)
* Step 2: enter variable with smallest p to enter.
    - Compute partial $R$ (or $\beta$) of $Y$ with each other $X$, given $X$ in, 
      and corresponding p-values (p to enter)
* Steps 3, 4, …: 
    - enter variable with smallest p to enter.
    - Compute partial R of Y with other X’s, given X’s in, and corresponding p to enter
* Repeat till smallest p to enter > cutoff value

**p-value cutoff?**

* See Bendel and Afifi (JASA, 1977)
    - Cutoff for p to enter = 0.15
    - Cutoff for p to remove = 0.30

**Example: Forward selection**

#### Backward Elimination
* Start with all X variables
* Step 0: Compute partial R of Y with each X, given all other X’s, 
  and corresponding p-values (p to remove)
* Step 1: remove variable with lowest |partial R| (highest p to remove)
    - Compute partial $R$ of $Y$ with each other $X$, given $X$ in, 
      and corresponding p-values (p to remove)
* Step 2: remove variable with highest p to remove.
    - Compute partial $R$ of $Y$ with each other $X$, given $X$ in, 
      and corresponding p-values (p to remove)
* Steps 3, 4, …: remove variable with highest p to remove.
    - Compute partial $R$ of $Y$ with each other $X$, given $X$ in, 
      and corresponding p-values (p to remove)
* Repeat till highest p to remove < cutoff value

**Example: Backward elimination**


#### Stepwise procedure

* Start with forward selection.
* At each step, look at variables in and check if you should remove some.
* Repeat till no $X$ can be added or removed



### Best Subset Regression
* Select one $X$ with highest simple $R$ with $Y$
* Select two $X$’s with highest multiple $R$ with $Y$
* Select three $X$’s with highest multiple $R$ with $Y$
etc.
* Compute CP, adjusted $R^{2}$, AIC or BIC each time.
* Compare and choose among the "best subsets" of various sizes.

**Example: Best Subsets**



#### Other resources for automated variable selection procedures in R.  

http://www.statmethods.net/stats/regression.html
http://www.stat.columbia.edu/~martin/W2024/R10.pdf  
https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/stepAIC.html
http://www.stat.colostate.edu/~darrenho/AMA/1_regression2.pdf


# Comments with automated variable selection methods 
* You can also enter and remove variables in blocks, 
  e.g., dummy variables representing a nominal $X$ should all be in 
  together, or all excluded together. 
* You can force some variables in.

# Model validation via cross-validation 
* Randomly split the sample into training sample (3/4) and validation sample 
  (1/4) or (2/3 vs 1/3)
* Compute regression equation from training sample
* Use that equation to compute predicted values in validation sample
* Compute simple $R$ of observed and predicted $Y$ in validation sample and 
  compare it with multiple $R$ in training sample
* Compare residuals in two samples
* You can also do this many times (1000?) and take the average $R$.


# What to watch out for
* Use previous research as a guide
* Variables not included can bias the results
* Significance levels are only a guide
* Perform diagnostics after selection
* _**Use common sense**_:
    - A suboptimal subset may make more sense than optimal one

In addition to the almost dozen entries in the textbook, see the following 
resources regarding areas of concern. 

http://www.stata.com/support/faqs/statistics/stepwise-regression-problems/
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.126.4133&rep=rep1&type=pdf

TLDR; Don't expect a magic bullet and don't use any method blindly. 



## Penalized methods
I won't go over these methods because we don't have the time to do them
justice. However I encourage you to learn more about methods such as
Ridge/Lasso regression, and cross-validation methods. 
Here are a few places to start. 

* http://www.stat.ucla.edu/~cocteau/stat120b/lectures/lecture7.pdf
* http://statweb.stanford.edu/~jtaylo/courses/stats203/notes/penalized.pdf
* http://www.stat.ufl.edu/archived/casella/Papers/BL-Final.pdf
* http://www.r-bloggers.com/variable-selection-using-cross-validation-and-other-techniques/


# On Your Own
##### On Your Own

1. Using the depression data set, create a model to predict CESD using the 
following set of potential predictors: age, income, level of education. 
    a. Use all four methods of automatic variable selection, and explain/
       justify your choice for entry/exit criteria. 
    b. Combine the parameter estimates and standard errors across all four
       models into a single table. Display this table in a nicely formatted
       method. Compare and discuss the differences and similarities across
       the four final models. 
2. For the lung function data, use an automated selection process to predict
   FEV1 for the oldest child using age, height, weight and FVC as candidate
   variables. State and justify the method and criteria you chose. 
3. Force the variables you selected in problem 2 into the regression equation
   with `OCFEV1` as the dependent variable, and test whether including the
   FEV1 of the parents (`MFEV1` and `FFEV1` taken as a pair) in the model
   significatly improves the regression. 
4. Using the Parental HIV data find the best model that predicts the age at
   which adolescents started drinking alcohol. Since the data were collected
   retrospectively, only consider variables which might be considered
   representative of the time before the adolescent started drinking alcohol.
  
    
    
    
    
    